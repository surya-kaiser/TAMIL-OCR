import os, random, shutil
from pathlib import Path

base = Path("data/train")
val_dir = Path("data/val")
val_dir.mkdir(parents=True, exist_ok=True)

random.seed(42)
val_ratio = 0.1  # 10% of training data

for cls in base.iterdir():
    if not cls.is_dir():
        continue
    images = list(cls.glob("*"))
    random.shuffle(images)
    n_val = int(len(images) * val_ratio)
    val_cls_dir = val_dir / cls.name
    val_cls_dir.mkdir(parents=True, exist_ok=True)
    for img in images[:n_val]:
        shutil.move(str(img), str(val_cls_dir / img.name))
    print(f"{cls.name}: moved {n_val} images to val")

# make_class_map.py
import csv, argparse, os
from pathlib import Path

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data-dir", required=True)  # e.g. F:\Tamil_OCR_Project\data
    ap.add_argument("--out", default="class_map.csv")
    args = ap.parse_args()

    train_dir = Path(args.data_dir) / "train"
    if not train_dir.exists():
        raise SystemExit(f"train folder not found: {train_dir}")

    class_ids = sorted([d.name for d in train_dir.iterdir() if d.is_dir()],
                       key=lambda x: int(x) if x.isdigit() else x)

    out = Path(args.out)
    with out.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["class_id","tamil_char"])
        for cid in class_ids:
            w.writerow([cid, ""])  # fill this later
    print(f"‚úî wrote template: {out.resolve()}")
    print("‚Üí open it and fill tamil_char for each class_id (use actual Tamil glyphs).")

if __name__ == "__main__":
    main()
# train_rtsba_tamil.py
# Tamil OCR - RTSBA model (GPU-ready)
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from pathlib import Path
import argparse

# -------------------------
# Non-local block
# -------------------------
class NonLocalBlock(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        inter = max(1, in_channels // 2)
        self.theta = nn.Conv2d(in_channels, inter, 1, bias=False)
        self.phi   = nn.Conv2d(in_channels, inter, 1, bias=False)
        self.g     = nn.Conv2d(in_channels, inter, 1, bias=False)
        self.out   = nn.Conv2d(inter, in_channels, 1, bias=False)
        self.bn    = nn.BatchNorm2d(in_channels)
    def forward(self, x):
        b, c, h, w = x.shape
        theta = self.theta(x).view(b, -1, h*w).transpose(1,2)
        phi   = self.phi(x).view(b, -1, h*w)
        g     = self.g(x).view(b, -1, h*w).transpose(1,2)
        attn  = torch.softmax(theta @ phi, dim=-1)
        y     = attn @ g
        y     = y.transpose(1,2).contiguous().view(b, -1, h, w)
        return self.bn(self.out(y)) + x

# -------------------------
# Attention Gate
# -------------------------
class AttentionGate(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        inter = max(1, in_channels // 2)
        self.conv1 = nn.Conv2d(in_channels, inter, 1)
        self.conv2 = nn.Conv2d(inter, 1, 1)
        self.sig   = nn.Sigmoid()
    def forward(self, x):
        att = F.relu(self.conv1(x))
        att = self.sig(self.conv2(att))
        return x * att

# -------------------------
# Bottleneck (ResNet-style)
# -------------------------
class Bottleneck(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        mid = out_ch // 4
        self.conv1 = nn.Conv2d(in_ch, mid, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid)
        self.conv2 = nn.Conv2d(mid, mid, 3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid)
        self.conv3 = nn.Conv2d(mid, out_ch, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_ch)
        self.down = None
        if stride != 1 or in_ch != out_ch:
            self.down = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_ch)
            )
        self.relu = nn.ReLU(inplace=True)
    def forward(self, x):
        idn = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        if self.down is not None:
            idn = self.down(x)
        out += idn
        return self.relu(out)

# -------------------------
# RTSBA Model
# -------------------------
class RTSBA(nn.Module):
    def __init__(self, n_classes, base=64):
        super().__init__()
        self.stem = nn.Sequential(
            nn.Conv2d(1, base, 7, 2, 3, bias=False),
            nn.BatchNorm2d(base),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, 2, 1)
        )
        self.enc1 = nn.Sequential(
            Bottleneck(base, base*4),
            Bottleneck(base*4, base*4)
        )
        self.enc2 = nn.Sequential(
            Bottleneck(base*4, base*8, stride=2),
            Bottleneck(base*8, base*8)
        )
        self.nonlocal_block = NonLocalBlock(base*8)
        self.att_gate = AttentionGate(base*8)
        self.up = nn.Sequential(
            nn.ConvTranspose2d(base*8, base*4, 2, 2),
            nn.BatchNorm2d(base*4),
            nn.ReLU(inplace=True),
            Bottleneck(base*4, base*4)
        )
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(base*4, n_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.enc1(x)
        x = self.enc2(x)
        x = self.nonlocal_block(x)
        x = self.att_gate(x)
        x = self.up(x)
        x = self.pool(x).flatten(1)
        return self.fc(x)

# -------------------------
# Training + Validation
# -------------------------
def run_epoch(model, loader, crit, opt, device, train=True):
    model.train(train)
    total, correct, loss_sum = 0, 0, 0.0
    ctx = torch.enable_grad() if train else torch.no_grad()
    with ctx:
        for imgs, labels in tqdm(loader, desc="train" if train else "val", leave=False):
            imgs, labels = imgs.to(device), labels.to(device)
            if train: opt.zero_grad()
            out = model(imgs)
            loss = crit(out, labels)
            if train:
                loss.backward()
                opt.step()
            loss_sum += loss.item() * imgs.size(0)
            correct += (out.argmax(1) == labels).sum().item()
            total += imgs.size(0)
    return loss_sum/total, correct/total

# -------------------------
# Main Training Loop
# -------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-dir", required=True)
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--batch-size", type=int, default=64)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--img-size", type=int, default=128)
    parser.add_argument("--save-path", default="best_rtsba.pth")
    args = parser.parse_args()

    train_tf = transforms.Compose([
        transforms.Grayscale(),
        transforms.Resize((args.img_size,args.img_size)),
        transforms.RandomRotation(6),
        transforms.RandomAffine(0, translate=(0.05,0.05), scale=(0.9,1.1)),
        transforms.ToTensor(),
        transforms.Normalize([0.5],[0.5])
    ])
    eval_tf = transforms.Compose([
        transforms.Grayscale(),
        transforms.Resize((args.img_size,args.img_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.5],[0.5])
    ])

    root = Path(args.data_dir)
    ds_train = datasets.ImageFolder(root/"train", transform=train_tf)
    ds_val = datasets.ImageFolder(root/"val", transform=eval_tf)
    ds_test = datasets.ImageFolder(root/"test", transform=eval_tf)

    print(f"Classes: {len(ds_train.classes)}")

    train_loader = DataLoader(ds_train, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(ds_val, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)
    test_loader = DataLoader(ds_test, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)

    model = RTSBA(len(ds_train.classes)).to(device)
    crit = nn.CrossEntropyLoss()
    opt = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)
    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=8, gamma=0.5)

    best_acc = 0
    for ep in range(1, args.epochs+1):
        print(f"\nEpoch {ep}/{args.epochs}")
        tr_loss, tr_acc = run_epoch(model, train_loader, crit, opt, device, train=True)
        vl_loss, vl_acc = run_epoch(model, val_loader, crit, opt, device, train=False)
        sched.step()
        print(f"Train: loss={tr_loss:.4f}, acc={tr_acc:.4f} | Val: loss={vl_loss:.4f}, acc={vl_acc:.4f}")
        if vl_acc > best_acc:
            best_acc = vl_acc
            torch.save({
                "state_dict": model.state_dict(),
                "classes": ds_train.classes,
                "img_size": args.img_size
            }, args.save_path)import torch
from torchvision import transforms
from PIL import Image
from train_rtsba_tamil import RTSBA

# -----------------------------
# Load trained model checkpoint
# -----------------------------
ckpt = torch.load("F:/Tamil_OCR_Project/tamil_ocr_model.pth", map_location="cuda")

# Recreate model with same class count
model = RTSBA(n_classes=len(ckpt["classes"]))
model.load_state_dict(ckpt["state_dict"])
model = model.cuda()
model.eval()

# -----------------------------
# Define preprocessing (same as training)
# -----------------------------
transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.Resize((ckpt["img_size"], ckpt["img_size"])),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# -----------------------------
# Load and preprocess a test image
# -----------------------------
img_path = r"F:\Tamil_OCR_Project\data\test\8\8136.bmp"   # change as needed
img = Image.open(img_path).convert("RGB")
img_tensor = transform(img).unsqueeze(0).cuda()

# -----------------------------
# Run inference
# -----------------------------
with torch.no_grad():
    output = model(img_tensor)
    pred_idx = output.argmax(1).item()

classes = ckpt["classes"]
pred_class = classes[pred_idx]


# -----------------------------
# Print results
# -----------------------------
print(f"üñºÔ∏è Image: {img_path}")
print(f"Predicted Class Index: {pred_idx}")
print(f"Predicted Folder: {pred_class}")
print(f"üéØ Predicted Tamil Letter: {tamil_character}")

            print("‚úÖ Saved best model:", args.save_path)

    print("\nLoading best model for testing...")
    ckpt = torch.load(args.save_path, map_location=device)
    model.load_state_dict(ckpt["state_dict"])
    test_loss, test_acc = run_epoch(model, test_loader, crit, opt, device, train=False)
    print(f"Test Accuracy: {test_acc:.4f}")

if __name__ == "__main__":
    main()
